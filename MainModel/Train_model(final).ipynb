{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_model(final).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRv2EyNDKmcL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t04RcenKK06f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    EncoderDecoderModel,\n",
        "    BertTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "uzFeC_yaKvJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive import"
      ],
      "metadata": {
        "id": "mw-iDVf3_d7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/MyDrive/project3/'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xkrcncHMNJ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Load & Split"
      ],
      "metadata": {
        "id": "CIpiJubR_owh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer(drive_path + f'dump/wpm-vocab-extend-30522.txt', do_lower_case=False)"
      ],
      "metadata": {
        "id": "0fwI7XDGK9jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def make_corpus(src_file_path, trg_file_path, file_type: str):\n",
        "    data = []\n",
        "\n",
        "    if file_type == 'jit':\n",
        "\n",
        "        with open(src_file_path, 'r') as fd:\n",
        "                for line in fd.readlines():\n",
        "                    data.append([line[:-1]])\n",
        "\n",
        "        with open(trg_file_path, 'r') as fd:\n",
        "            for i, line in enumerate(fd.readlines()):\n",
        "                data[i].append(line[:-1])\n",
        "\n",
        "    elif file_type == 'pickled_ai_hub':\n",
        "\n",
        "        with open(src_file_path, 'rb') as f:\n",
        "            tmp_src = pickle.load(f)\n",
        "\n",
        "        with open(trg_file_path, 'rb') as f:\n",
        "            tmp_trg = pickle.load(f)\n",
        "            \n",
        "        for i in range(len(tmp_src)):\n",
        "            data.append([tmp_src[i], tmp_trg[i]])\n",
        "    return data"
      ],
      "metadata": {
        "id": "ThymLPFrgKM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Korea to Jeju\n",
        "ai_hub_data = make_corpus(drive_path + '전처리된_AI_HUB_data/ai_hub_standard_v2.pkl', drive_path + '전처리된_AI_HUB_data/ai_hub_trans_v2.pkl', 'pickled_ai_hub')\n",
        "train_ai_hub = ai_hub_data[:-10000]\n",
        "dev_ai_hub = ai_hub_data[-10000:-5000]\n",
        "test_ai_hub = ai_hub_data[-5000:]\n",
        "train_data = make_corpus(drive_path + 'jit/ko.train', drive_path + 'jit/je.train', 'jit')\n",
        "dev_data = make_corpus(drive_path + 'jit/ko.dev', drive_path + 'jit/je.dev', 'jit')\n",
        "test_data = make_corpus(drive_path + 'jit/ko.test', drive_path + 'jit/je.test', 'jit')\n",
        "\n",
        "# Train Jeju to Korea\n",
        "''' \n",
        "ai_hub_data = make_corpus(drive_path + '전처리된_AI_HUB_data/ai_hub_trans_v2.pkl', drive_path + '전처리된_AI_HUB_data/ai_hub_standard_v2.pkl', 'pickled_ai_hub')\n",
        "train_ai_hub = ai_hub_data[:-10000]\n",
        "dev_ai_hub = ai_hub_data[-10000:-5000]\n",
        "test_ai_hub = ai_hub_data[-5000:]\n",
        "train_data = make_corpus(drive_path + 'jit/je.train', drive_path + 'jit/ko.train', 'jit')\n",
        "dev_data = make_corpus(drive_path + 'jit/je.dev', drive_path + 'jit/ko.dev', 'jit')\n",
        "test_data = make_corpus(drive_path + 'jit/je.test', drive_path + 'jit/ko.test', 'jit')\n",
        "'''"
      ],
      "metadata": {
        "id": "Z88AajuyjjPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dev_data)"
      ],
      "metadata": {
        "id": "wv3JBcillu8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data + train_ai_hub\n",
        "dev_data = dev_data + dev_ai_hub\n",
        "test_data = test_data + test_ai_hub\n",
        "\n",
        "len(train_data), len(dev_data), len(test_data)"
      ],
      "metadata": {
        "id": "DEicLIYVmLsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Tokenizing"
      ],
      "metadata": {
        "id": "A1wpiU-sLq-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(tokenizer, data, max_length=128, stride=30):\n",
        "    \n",
        "    cnt = 0\n",
        "    embeddings = []\n",
        "    for src, trg in data:\n",
        "        src_sample = tokenizer(src, truncation=True, max_length=max_length, stride=stride, return_token_type_ids=False, return_attention_mask=False, return_overflowing_tokens=True)\n",
        "        trg_sample = tokenizer(trg, truncation=True, max_length=max_length, stride=stride, return_token_type_ids=False, return_attention_mask=False, return_overflowing_tokens=True)\n",
        "        embeddings.append({'input_ids' : src_sample['input_ids'],\n",
        "                           'labels': trg_sample['input_ids']})\n",
        "        if src_sample['num_truncated_tokens'] > 0 and trg_sample['num_truncated_tokens'] > 0:\n",
        "            src_tmp = src_sample['overflowing_tokens']\n",
        "            trg_tmp = trg_sample['overflowing_tokens']\n",
        "            while len(src_tmp) > 0 and len(trg_tmp) > 0:\n",
        "                cnt += 1\n",
        "                src_input = [tokenizer.cls_token_id]\n",
        "                trg_input = [tokenizer.cls_token_id]\n",
        "                src_input.extend(src_tmp[:max_length-2])\n",
        "                src_input.append(tokenizer.sep_token_id)\n",
        "                trg_input.extend(trg_tmp[:max_length-2])\n",
        "                trg_input.append(tokenizer.sep_token_id)                \n",
        "                embeddings.append({'input_ids' : src_input,\n",
        "                               'labels': trg_input})\n",
        "                src_tmp = src_tmp[max_length-stride-2:2*max_length-stride-2]\n",
        "                trg_tmp = trg_tmp[max_length-stride-2:2*max_length-stride-2]\n",
        "    print(f'Processed {cnt} amount of overflowing token set!')\n",
        "\n",
        "    cnt = 0\n",
        "    for item in embeddings:\n",
        "        if len(item['input_ids']) == 2 or len(item['labels']) == 2:\n",
        "            embeddings.remove(item)\n",
        "            cnt += 1\n",
        "    print(f'Removed {cnt} amount of empty token set!')\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "AJN9odV8WEz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized_data = tokenized_data(tokenizer, train_data)\n",
        "dev_tokenized_data = tokenized_data(tokenizer, dev_data)\n",
        "test_tokenized_data = tokenized_data(tokenizer, test_data)"
      ],
      "metadata": {
        "id": "tp25kMAfYMPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, features):\n",
        "        super(DatasetRetriever, self).__init__()\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, index):   \n",
        "        feature = self.features[index]\n",
        "        return {\n",
        "            'input_ids':torch.tensor(feature['input_ids'] ,dtype=torch.long),\n",
        "            'labels':torch.tensor(feature['labels'] ,dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "jiwWo0fkwbiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DatasetRetriever(train_tokenized_data)\n",
        "dev_dataset = DatasetRetriever(dev_tokenized_data)\n",
        "test_dataset = DatasetRetriever(test_tokenized_data)"
      ],
      "metadata": {
        "id": "BmeBp2IL2pfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "Do_RN6bcLvHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "\n",
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "config_encoder = BertConfig()\n",
        "config_decoder = BertConfig()\n",
        "\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
        "\n",
        "# Initializing a Bert2Bert model from the bert-base-uncased style configurations\n",
        "model = EncoderDecoderModel(config=config)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Accessing the model configuration\n",
        "config_encoder = model.config.encoder\n",
        "config_decoder = model.config.decoder\n",
        "# set decoder config to causal lm\n",
        "config_encoder.bos_token_id = tokenizer.cls_token_id\n",
        "config_encoder.eos_token_id = tokenizer.sep_token_id\n",
        "config_encoder.decoder_start_token_id = tokenizer.cls_token_id\n",
        "config_decoder.is_decoder = True\n",
        "config_decoder.add_cross_attention = True\n",
        "config_decoder.bos_token_id = tokenizer.cls_token_id\n",
        "config_decoder.eos_token_id = tokenizer.sep_token_id\n",
        "config_decoder.decoder_start_token_id = tokenizer.cls_token_id"
      ],
      "metadata": {
        "id": "-1lfoVvm7Py6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model on GPU"
      ],
      "metadata": {
        "id": "SHGrFefdAPlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "cwGbbFszH_H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "b_sOQTbaL2mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorForSeq2Seq(tokenizer, model)\n",
        "\n",
        "arguments = Seq2SeqTrainingArguments(\n",
        "    output_dir= drive_path + 'dump/models',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    save_total_limit=5,\n",
        "    dataloader_num_workers=1,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    arguments,\n",
        "    data_collator=collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "VKM5FbprM2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(drive_path + \"dump/models\")"
      ],
      "metadata": {
        "id": "fONzE62mNEeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
