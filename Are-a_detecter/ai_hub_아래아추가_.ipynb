{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_hub_아래아추가.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/MyDrive/project3/'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#/content/drive/MyDrive/project3/dump/주형준_vocab/wpm-vocab-je.txt"
      ],
      "metadata": {
        "id": "e_fY1IruTvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qPcP_BlQEC7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(drive_path + '전처리된_AI_HUB_data/Train/전처리된_AI_hub_train.csv')\n",
        "df_train.dropna(inplace=True)\n",
        "df_dev = pd.read_csv(drive_path + '전처리된_AI_HUB_data/Val/전처리된_AI_hub_val.csv')\n",
        "df_dev.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train)"
      ],
      "metadata": {
        "id": "fV0T26KRQQsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ai-hub 데이터는 dialect와 standard가 같은 경우가 있다. 둘이 다른 경우만 사용할 수 있도록 데이터를 선별하자."
      ],
      "metadata": {
        "id": "7L1ecNsJQUGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition = df_train['dialect'] != df_train['standard']\n",
        "df_train = df_train[condition]"
      ],
      "metadata": {
        "id": "iyyJscc_QRkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train)"
      ],
      "metadata": {
        "id": "p1jhANb4QV5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[:10]"
      ],
      "metadata": {
        "id": "vsE9JeXZQWiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_standard = df_train['standard']\n",
        "df_train_dialect = df_train['dialect']\n",
        "\n",
        "test_df_train_standard = list(df_train_standard[:200000])\n",
        "test_df_train_dialect = list(df_train_dialect[:200000])"
      ],
      "metadata": {
        "id": "TNjWyPK7QXSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_df_train_dialect), len(test_df_train_standard))"
      ],
      "metadata": {
        "id": "6Ta8H25nQYTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_train_standard[:10]"
      ],
      "metadata": {
        "id": "8YPoIFunQZAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_train_dialect[:10]"
      ],
      "metadata": {
        "id": "G7qcCoj2QZ3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래아 추가하기"
      ],
      "metadata": {
        "id": "X4ceOZtyQaRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = BertTokenizer(drive_path + 'jit_for_AreA_train/dump/wpm-vocab-je_trans.txt', do_lower_case=False)\n",
        "trg_tokenizer = BertTokenizer(drive_path + 'jit_for_AreA_train/dump/wpm-vocab-je.txt', do_lower_case=False)"
      ],
      "metadata": {
        "id": "kLeeTjLxTLz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoderModel.from_pretrained(drive_path + 'jit_for_AreA_train/dump/model1').to(device)"
      ],
      "metadata": {
        "id": "cI2vMCqxVSrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.decoder_start_token_id = trg_tokenizer.cls_token_id\n",
        "model.config.pad_token_id = trg_tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "XV_H9m5PVTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text):\n",
        "    embeddings = src_tokenizer(text, return_attention_mask = False, return_token_type_ids= False, return_tensors = 'pt').to(device)\n",
        "    output = model.generate(**embeddings, max_length = 256, eos_token_id=3)[0, 1:-1].to(device)\n",
        "    return trg_tokenizer.decode(output[1:])"
      ],
      "metadata": {
        "id": "T3nYIPcRTM_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "ai_test_pred = []\n",
        "\n",
        "for i in tqdm(range(len(test_df_train_dialect))):\n",
        "    src = test_df_train_dialect[i]\n",
        "    output = get_prediction(src)\n",
        "    ai_test_pred.append(output)"
      ],
      "metadata": {
        "id": "yvWGEIRFQbyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 저장하기\n",
        "import pickle\n",
        "with open('ai_hub_trans.pkl', 'wb') as f:\n",
        "    pickle.dump(ai_test_pred, f)"
      ],
      "metadata": {
        "id": "mLsyUIrBQcwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 저장하기\n",
        "with open('ai_hub_standard.pkl', 'wb') as f:\n",
        "    pickle.dump(test_df_train_standard, f)"
      ],
      "metadata": {
        "id": "l32mKDE1QfnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w-FcchUqQgmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}